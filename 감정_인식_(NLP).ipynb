{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPrzS5NfxETnNsgGiCXglcy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taewon-Park/Dacon/blob/main/%EA%B0%90%EC%A0%95_%EC%9D%B8%EC%8B%9D_(NLP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmoFh8I6qql5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/dacon/발화자의 감정인식/data/open')"
      ],
      "metadata": {
        "id": "QtmV9-6RrN3r"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?=1-QNs8sk5X3u_1rK-dv5ESgQIt0ZDQSwY\n",
        "!unzip -qq \"./data/databffhw4rntmp\""
      ],
      "metadata": {
        "id": "eRYkr9TTxOb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Requirements\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import random\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, RobertaModel, AutoModel\n",
        "from transformers import BertModel, RobertaTokenizer, AutoTokenizer\n",
        "from torch.optim import Adam\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "N_74q1SuquLn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Data Load\n",
        "data = pd.read_csv(\"train.csv\")"
      ],
      "metadata": {
        "id": "rOVANQ8ZrUED"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "CFG = {\n",
        "  'EPOCHS' : 50,\n",
        "  'LEARNING_RATE' : 1e-5,\n",
        "  'BATCH_SIZE' : 2,\n",
        "  'SEED' : 42\n",
        "}\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "  random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "def Encoding(data):\n",
        "  le = LabelEncoder()\n",
        "  le = le.fit(data['Target'])\n",
        "  data['Target'] = le.transform(data['Target'])\n",
        "\n",
        "  return data['Target']\n",
        "\n",
        "def Class_Weights(data):\n",
        "  class_counts = data['Target'].value_counts()\n",
        "  class_weights = 1./class_counts\n",
        "  class_weights = class_weights/class_weights.min()\n",
        "  class_weights = class_weights.to_dict()\n",
        "  class_weights = {k : v for k, v in sorted(class_weights.items(), key=lambda item : item[0])}\n",
        "  class_weights = list(class_weights.values())\n",
        "  class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "\n",
        "  return class_weights\n",
        "\n",
        "def Strat_Split(nsplit, data):\n",
        "\n",
        "  folds = StratifiedKFold(n_splits=nsplit, shuffle=True, random_state=CFG['SEED'])\n",
        "  data['fold'] = -1\n",
        "  for i in range(nsplit):\n",
        "    trn_idx, val_idx = list(folds.split(data, data['Target']))[i]\n",
        "    valid = data.iloc[val_idx]\n",
        "    data.loc[data[data.ID.isin(valid.ID) == True].index.to_list(), 'fold'] = i\n",
        "\n",
        "  data.to_csv('train_fold.csv', index=False)\n",
        "  fold = pd.read_csv(\"train_fold.csv\")\n",
        "\n",
        "  le = LabelEncoder()\n",
        "  le = le.fit(fold['Target'])\n",
        "  fold['Target'] = le.transform(fold['Target'])\n",
        "\n",
        "  strat_train = fold[fold['fold'] != 1].reset_index(drop=True)\n",
        "  strat_valid = fold[fold['fold'] == 1].reset_index(drop=True)\n",
        "\n",
        "  return strat_train, strat_valid, le\n",
        "\n",
        "\n",
        "def Tokenizer_Define():\n",
        "  bert = AutoModel.from_pretrained(\"tae898/emoberta-large\").to(device)\n",
        "  bert_pool = bert.pooler\n",
        "  bert.pooler = torch.nn.Identity()\n",
        "\n",
        "  return bert, bert_pool\n",
        "\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, data, mode = \"train\"):\n",
        "    self.dataset = data\n",
        "    self.mode = mode\n",
        "    self.feature = []\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"tae898/emoberta-large\")\n",
        "\n",
        "    for text in tqdm(data['Utterance']):\n",
        "      inputs = tokenizer(text, padding='max_length', max_length=328, truncation=True, return_tensors=\"pt\")\n",
        "      input_ids = inputs['input_ids'][0][None].to(device)\n",
        "      attention_mask = inputs['attention_mask'][0][None].to(device)\n",
        "\n",
        "      _, pooled_output = bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
        "      self.feature.append(pooled_output.detach()[0])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if self.mode == \"train\":\n",
        "      return self.feature[idx], self.dataset['Target'][idx]\n",
        "    else:\n",
        "      return self.feature[idx]"
      ],
      "metadata": {
        "id": "rWNhAQQ7rBxp"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Preprocessing\n",
        "seed_everything(CFG['SEED'])\n",
        "\n",
        "Encoding(data)\n",
        "class_weights = Class_Weights(data)\n",
        "strat_train, strat_valid, le = Strat_Split(35, data)\n",
        "\n",
        "bert, bert_pool = Tokenizer_Define()"
      ],
      "metadata": {
        "id": "mKIu1EX7rtnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratified Dataset Check\n",
        "print(\"train set : \", len(strat_train))\n",
        "print(\"test set : \", len(strat_valid))"
      ],
      "metadata": {
        "id": "fZQQvpulsm0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make CustomDataset & DataLoader\n",
        "train_ds = CustomDataset(strat_train, mode = \"train\")\n",
        "valid_ds = CustomDataset(strat_valid, mode = \"train\")\n",
        "train_dataloader = torch.utils.data.DataLoader(train_ds, batch_size= CFG['BATCH_SIZE'], shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(valid_ds, batch_size= CFG['BATCH_SIZE'], shuffle=True)"
      ],
      "metadata": {
        "id": "tc5poDGAtUEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modeling\n",
        "class BaseModel(nn.Module):\n",
        "  def __init__(self, dropout=0.5, num_classes=len(le.classes_)):\n",
        "    super(BaseModel, self).__init__()\n",
        "    self.bert_pool = bert_pool\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear = nn.Sequential(\n",
        "      nn.Linear(1024, 512),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(512, num_classes),\n",
        "    )\n",
        "\n",
        "  def forward(self, pooled_output):\n",
        "    pooled_output = self.bert_pool(pooled_output)\n",
        "    dropout_output = self.dropout(pooled_output)\n",
        "    linear_output = self.linear(pooled_output)\n",
        "\n",
        "    return linear_output\n",
        "\n",
        "\n",
        "def train(model, optimizer, train_loader, test_loader, scheduler, class_weights, device):\n",
        "  model.to(device)\n",
        "  criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
        "\n",
        "  best_score = 0\n",
        "  best_model = \"None\"\n",
        "  for epoch_num in range(CFG[\"EPOCHS\"]):\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    for pooled_output, train_label in tqdm(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      pooled_output = pooled_output.to(device)\n",
        "      train_label = train_label.to(device)\n",
        "\n",
        "      output = model(pooled_output).to(device)\n",
        "      batch_loss = criterion(output, train_label.long())\n",
        "\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss.append(batch_loss.item())\n",
        "\n",
        "    val_loss, val_score = validation(model, criterion, test_loader, device)\n",
        "    print(f'Epoch [{epoch_num}], Train Loss : [{np.mean(train_loss) :.5f}] Val Loss : [{np.mean(val_loss) :.5f}] Val F1 Score : [{val_score : .5f}]')\n",
        "    if scheduler is not None:\n",
        "      scheduler.step(val_score)\n",
        "\n",
        "    best_model = model\n",
        "    best_score = val_score\n",
        "\n",
        "  return best_model\n",
        "\n",
        "\n",
        "def competition_metric(true, pred):\n",
        "  return f1_score(true, pred, average=\"macro\")\n",
        "\n",
        "\n",
        "def validation(model, criterion, test_loader, device):\n",
        "  model.eval()\n",
        "\n",
        "  val_loss = []\n",
        "  model_preds = []\n",
        "  true_labels = []\n",
        "  with torch.no_grad():\n",
        "    for pooled_output, valid_label in tqdm(test_loader):\n",
        "      valid_label = valid_label.to(device)\n",
        "      pooled_output = pooled_output.to(device)\n",
        "\n",
        "      output = model(pooled_output).to(device)\n",
        "\n",
        "      batch_loss = criterion(output, valid_label.long())\n",
        "      val_loss.append(batch_loss.item())\n",
        "\n",
        "      model_preds += output.argmax(1).detach().cpu().numpy().tolist()\n",
        "      true_labels += valid_label.detach().cpu().numpy().tolist()\n",
        "      val_f1 = competition_metric(true_labels, model_preds)\n",
        "\n",
        "  return val_loss, val_f1"
      ],
      "metadata": {
        "id": "oXqbtbtPrE3n"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "model = BaseModel()\n",
        "model.eval()\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n",
        "infer_model = train(model, optimizer, train_dataloader, val_dataloader, scheduler, class_weights, device)"
      ],
      "metadata": {
        "id": "FxeY8zCWvonj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model.pt')"
      ],
      "metadata": {
        "id": "0g5e8T8G0BYe"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(infer_model.state_dict(), 'infer_model.pt')"
      ],
      "metadata": {
        "id": "CuMabEPgzCT9"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ESn84KG2ysK1"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Data Load\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "test_ds = CustomDataset(test, mode = \"test\")\n",
        "test_dataloader = torch.utils.data.DataLoader(test_ds, batch_size = CFG['BATCH_SIZE'], shuffle=False)"
      ],
      "metadata": {
        "id": "aE-HWEnowkcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_predict = []\n",
        "    for pooled_output in tqdm(test_loader):\n",
        "        pooled_output = pooled_output.to(device)\n",
        "        y_pred = model(pooled_output)\n",
        "        test_predict += y_pred.argmax(1).detach().cpu().numpy().tolist()\n",
        "    print('Done.')\n",
        "\n",
        "    return test_predict"
      ],
      "metadata": {
        "id": "DwoVx9ILwP6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = inference(infer_model, test_dataloader, device)\n",
        "preds = le.inverse_transfrom(preds)\n",
        "\n",
        "submit = pd.read_csv('sample_submission.csv')\n",
        "submit['Target'] = preds\n",
        "\n",
        "# submit에서 Target이 0이면 anger로 변환\n",
        "submit['Target'] = submit['Target'].apply(lambda x: 'anger' if x == 0 else x)\n",
        "submit['Target'] = submit['Target'].apply(lambda x: 'disgust' if x == 1 else x)\n",
        "submit['Target'] = submit['Target'].apply(lambda x: 'fear' if x == 2 else x)\n",
        "submit['Target'] = submit['Target'].apply(lambda x: 'joy' if x == 3 else x)\n",
        "submit['Target'] = submit['Target'].apply(lambda x: 'neutral' if x == 4 else x)\n",
        "submit['Target'] = submit['Target'].apply(lambda x: 'sadness' if x == 5 else x)\n",
        "submit['Target'] = submit['Target'].apply(lambda x: 'surprise' if x == 6 else x)\n",
        "\n",
        "submit.to_csv('submit.csv', index=False)"
      ],
      "metadata": {
        "id": "tUD-NAVaw5Ey"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}