{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvv02vYoku0syH3xD9gcSa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taewon-Park/Dacon/blob/main/%EA%B0%90%EC%A0%95%EC%9D%B8%EC%8B%9D(Audio).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1IiRHjGI_moPgQUc4OtL53gB3xKClnQNN\n",
        "!unzip open.zip"
      ],
      "metadata": {
        "id": "x7fntsVzhAqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6fBNyhv0ayY"
      },
      "outputs": [],
      "source": [
        "# Requirements\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers.optimization import AdamW, get_constant_schedule_with_warmup\n",
        "from pytorch_lightning.utilities.seed import seed_everything\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, StochasticWeightAveraging\n",
        "from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer, AutoFeatureExtractor, HubertForSequenceClassification, AutoConfig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(preds, labels):\n",
        "  return (preds == labels).float().mean()\n",
        "\n",
        "\n",
        "def getAudios(df):\n",
        "  audios = []\n",
        "  for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    audio,_ = librosa.load(row['path'], sr=SAMPLING_RATE)\n",
        "    audios.append(audio)\n",
        "  return audios\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, audio, audio_feature_extractor, label = None):\n",
        "    if label is None:\n",
        "        label = [0] * len(audio)\n",
        "    self.label = np.array(label).astype(np.int64)\n",
        "    self.audio = audio\n",
        "    self.audio_feature_extractor = audio_feature_extractor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    label = self.label[idx]\n",
        "    audio = self.audio[idx]\n",
        "    audio_feature = audio_feature_extractor(raw_speech = audio, return_tensors = 'np', sampling_rate = SAMPLING_RATE)\n",
        "    audio_values, audio_attn_mask = audio_feature['input_values'][0], audio_feature['attention_mask'][0]\n",
        "\n",
        "    item = {\n",
        "        'label' : label\n",
        "        'audio_values' : audio_values,\n",
        "        'audio_attn_mask' : audio_attn_mask,\n",
        "    }\n",
        "\n",
        "    return item\n",
        "\n",
        "\n",
        "def collate_fn(samples):\n",
        "  batch_labels = []\n",
        "  batch_audio_values = []\n",
        "  batch_audio_attn_masks = []\n",
        "\n",
        "  for sample in samples:\n",
        "    batch_labels.append(sample['label'])\n",
        "    batch_audio_values.append(torch.tensro(sample['audio_values']))\n",
        "    batch_audio_attn_masks.append(torch.tensor(sample['audio_attn_mask']))\n",
        "\n",
        "  batch_labels = torch.tensor(batch_labels)\n",
        "  batch_audio_values = pad_sequence(batch_audio_values, batch_first = True)\n",
        "  batch_audio_attn_masks = pad_sequence(batch_audio_attn_masks, batch_first = True)\n",
        "\n",
        "  batch = {\n",
        "      'label' : batch_labels,\n",
        "      'audio_values' : batch_audio_values,\n",
        "      'audio_attn_mask' : batch_audio_attn_masks,\n",
        "  }\n",
        "\n",
        "  return batch\n",
        "\n",
        "\n",
        "class MyLitModel(pl.LightningModule):\n",
        "  def __init__(self, audio_model_name, num_labels, n_layers=1, projector=True, classifier=True, dropout=0.07, lr_decay=1):\n",
        "    super(MyLitModel, self).__init__()\n",
        "    self.config = AutoConfig.from_pretrained(audio_model_name)\n",
        "    self.config.activation_dropout = dropout\n",
        "    self.config.attention_dropout = dropout\n",
        "    self.config.final_dropout = dropout\n",
        "    self.config.hidden_dropout = dropout\n",
        "    self.config.hidden_dropout_prob = dropout\n",
        "    self.audio_model = HubertForSequenceClassification.from_pretrained(audio_model_name, config=self.config)\n",
        "    self.lr_decay = lr_decay\n",
        "    self._do_reinit(n_layers, projector, classifier)\n",
        "\n",
        "  def forward(self, audio_values, audio_attn_mask):\n",
        "    logits = self.audio_model(input_values = audio_values, attention_mask = audio_attn_mask).logits\n",
        "    logits = torch.stack([\n",
        "        logits[:, 0] + logits[:, 7],\n",
        "        logits[:, 2] + logits[:, 9],\n",
        "        logits[:, 5] + logits[:, 12],\n",
        "        logits[:, 1] + logits[:, 8],\n",
        "        logits[:, 4] + logits[:, 11],\n",
        "        logits[:, 3] + logits[:, 10],]\n",
        "        , dim = -1)\n",
        "    return logits\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    audio_values = batch['audio_values']\n",
        "    audio_attn_mask = batch['audio_attn_mask']\n",
        "    labels = batch['label']\n",
        "\n",
        "    logits = self(audio_values, audio_attn_mask)\n",
        "    loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    acc = accuracy(preds, labels)\n",
        "\n",
        "    self.log('train_loss', loss, on_step = True,  on_epoch = True, prog_bar = True, logger = True)\n",
        "    self.log('train_acc', acc, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def validation_step(self, batch, batch_idx):\n",
        "    audio_values = batch['audio_values']\n",
        "    audio_attn_mask = batch['audio_attn_mask']\n",
        "    labels = batch['label']\n",
        "\n",
        "    logits = self(audio_values, audio_attn_mask)\n",
        "    loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    acc = accuracy(preds, labels)\n",
        "\n",
        "    self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "    self.log('val_acc', acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
        "    audio_values = batch['audio_values']\n",
        "    audio_attn_mask = batch['audio_attn_mask']\n",
        "\n",
        "    logits = self(audio_values, audio_attn_mask)\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "    return preds\n",
        "\n",
        "def configure_optimizers(self):\n",
        "    lr = 1e-5\n",
        "    layer_decay = self.lr_decay\n",
        "    weight_decay = 0.01\n",
        "    llrd_params = self._get_llrd_params(lr=lr, layer_decay=layer_decay, weight_decay=weight_decay)\n",
        "    optimizer = AdamW(llrd_params)\n",
        "    return optimizer\n",
        "\n",
        "def _get_llrd_params(self, lr, layer_decay, weight_decay):\n",
        "    n_layers = self.audio_model.config.num_hidden_layers\n",
        "    llrd_params = []\n",
        "    for name, value in list(self.named_parameters()):\n",
        "        if ('bias' in name) or ('layer_norm' in name):\n",
        "            llrd_params.append({\"params\": value, \"lr\": lr, \"weight_decay\": 0.0})\n",
        "        elif ('emb' in name) or ('feature' in name) :\n",
        "            llrd_params.append({\"params\": value, \"lr\": lr * (layer_decay**(n_layers+1)), \"weight_decay\": weight_decay})\n",
        "        elif 'encoder.layer' in name:\n",
        "            for n_layer in range(n_layers):\n",
        "                if f'encoder.layer.{n_layer}' in name:\n",
        "                    llrd_params.append({\"params\": value, \"lr\": lr * (layer_decay**(n_layer+1)), \"weight_decay\": weight_decay})\n",
        "        else:\n",
        "            llrd_params.append({\"params\": value, \"lr\": lr , \"weight_decay\": weight_decay})\n",
        "    return llrd_params\n",
        "\n",
        "def _do_reinit(self, n_layers=0, projector=True, classifier=True):\n",
        "    if projector:\n",
        "        self.audio_model.projector.apply(self._init_weight_and_bias)\n",
        "    if classifier:\n",
        "        self.audio_model.classifier.apply(self._init_weight_and_bias)\n",
        "\n",
        "    for n in range(n_layers):\n",
        "        self.audio_model.hubert.encoder.layers[-(n+1)].apply(self._init_weight_and_bias)\n",
        "\n",
        "def _init_weight_and_bias(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        module.weight.data.normal_(mean=0.0, std=self.audio_model.config.initializer_range)\n",
        "        if module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "    elif isinstance(module, nn.LayerNorm):\n",
        "        module.bias.data.zero_()\n",
        "        module.weight.data.fill_(1.0)"
      ],
      "metadata": {
        "id": "4HJa2olh1Qvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = './data'\n",
        "PREPROC_DIR = './preproc'\n",
        "SUBMISSION_DIR = './submission'\n",
        "MODEL_DIR = './model'\n",
        "SAMPLING_RATE = 16000\n",
        "SEED=0\n",
        "N_FOLD=20\n",
        "BATCH_SIZE=8\n",
        "NUM_LABELS = 6"
      ],
      "metadata": {
        "id": "wo3dfxtKgtlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(SEED)"
      ],
      "metadata": {
        "id": "ibMVp_W6guhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_model_name = 'Rajaram1996/Hubert_emotion'\n",
        "audio_feature_extractor = AutoFeatureExtractor.from_pretrained(audio_model_name)\n",
        "audio_feature_extractor.return_attention_mask=True"
      ],
      "metadata": {
        "id": "53hP6iYUhots"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
        "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
        "train_df['path'] = train_df['path'].apply(lambda x: os.path.join(DATA_DIR, x))\n",
        "test_df['path'] = test_df['path'].apply(lambda x: os.path.join(DATA_DIR, x))\n",
        "train_audios = getAudios(train_df)\n",
        "test_audios = getAudios(test_df)\n",
        "train_label = train_df['label'].values"
      ],
      "metadata": {
        "id": "m33bh_PBhrMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits=N_FOLD,shuffle=True,random_state=SEED)\n",
        "for fold_idx, (train_indices, val_indices) in enumerate(skf.split(train_label, train_label)):\n",
        "    train_fold_audios = [train_audios[train_index] for train_index in train_indices]\n",
        "    val_fold_audios= [train_audios[val_index] for val_index in val_indices]\n",
        "\n",
        "    train_fold_label = train_label[train_indices]\n",
        "    val_fold_label = train_label[val_indices]\n",
        "    train_fold_ds = MyDataset(train_fold_audios, audio_feature_extractor, train_fold_label)\n",
        "    val_fold_ds = MyDataset(val_fold_audios, audio_feature_extractor, val_fold_label)\n",
        "    train_fold_dl = DataLoader(train_fold_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "    val_fold_dl = DataLoader(val_fold_ds, batch_size=BATCH_SIZE*2, collate_fn=collate_fn)\n",
        "\n",
        "    checkpoint_acc_callback = ModelCheckpoint(\n",
        "        monitor='val_acc',\n",
        "        dirpath=MODEL_DIR,\n",
        "        filename=f'{fold_idx=}'+'_{epoch:02d}-{val_acc:.4f}-{train_acc:.4f}',\n",
        "        save_top_k=1,\n",
        "        mode='max'\n",
        "    )\n",
        "\n",
        "    my_lit_model = MyLitModel(\n",
        "        audio_model_name=audio_model_name,\n",
        "        num_labels=NUM_LABELS,\n",
        "        n_layers=1, projector=True, classifier=True, dropout=0.07, lr_decay=0.8\n",
        "    )\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        accelerator='cuda',\n",
        "        max_epochs=30,\n",
        "        precision=16,\n",
        "        val_check_interval=0.1,\n",
        "        callbacks=[checkpoint_acc_callback],\n",
        "    )\n",
        "\n",
        "    trainer.fit(my_lit_model, train_fold_dl, val_fold_dl)\n",
        "\n",
        "    del my_lit_model"
      ],
      "metadata": {
        "id": "WH5aO0VZhtLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = MyDataset(test_audios, audio_feature_extractor)\n",
        "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE*2, collate_fn=collate_fn)\n",
        "pretrained_models = list(map(lambda x: os.path.join(MODEL_DIR,x),os.listdir(MODEL_DIR)))"
      ],
      "metadata": {
        "id": "nr8fJbHehweN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds = []\n",
        "trainer = pl.Trainer(\n",
        "    accelerator='cuda',\n",
        "    precision=16,\n",
        ")\n",
        "for pretrained_model_path in pretrained_models:\n",
        "    pretrained_model = MyLitModel.load_from_checkpoint(\n",
        "        pretrained_model_path,\n",
        "        audio_model_name=audio_model_name,\n",
        "        num_labels=NUM_LABELS,\n",
        "    )\n",
        "    test_pred = trainer.predict(pretrained_model, test_dl)\n",
        "    test_pred = torch.cat(test_pred).detach().cpu().numpy()\n",
        "    test_preds.append(test_pred)\n",
        "    del pretrained_model"
      ],
      "metadata": {
        "id": "W8D54WN3hx2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df = pd.read_csv(os.path.join(DATA_DIR,'sample_submission.csv'))\n",
        "submission_df['label'] = list(map(np.argmax,(map(np.bincount,np.array(test_preds).T))))\n",
        "submission_df.to_csv(os.path.join(SUBMISSION_DIR,'20_fold.csv'),index=False)"
      ],
      "metadata": {
        "id": "K7VyTPNhhyt0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}