{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2zsM+hZ3nhJJbfNyqpERe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taewon-Park/Dacon/blob/main/%EA%B0%90%EC%A0%95%EC%9D%B8%EC%8B%9D(Audio).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6fBNyhv0ayY"
      },
      "outputs": [],
      "source": [
        "# Requirements\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers.optimization import AdamW, get_constant_schedule_with_warmup\n",
        "from pytorch_lightning.utilities.seed import seed_everything\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, StochasticWeightAveraging\n",
        "from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer, AutoFeatureExtractor, HubertForSequenceClassification, AutoConfig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(preds, labels):\n",
        "  return (preds == labels).float().mean()\n",
        "\n",
        "\n",
        "def getAudios(df):\n",
        "  audios = []\n",
        "  for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    audio,_ = librosa.load(row['path'], sr=SAMPLING_RATE)\n",
        "    audios.append(audio)\n",
        "  return audios\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, audio, audio_feature_extractor, label = None):\n",
        "    if label is None:\n",
        "        label = [0] * len(audio)\n",
        "    self.label = np.array(label).astype(np.int64)\n",
        "    self.audio = audio\n",
        "    self.audio_feature_extractor = audio_feature_extractor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    label = self.label[idx]\n",
        "    audio = self.audio[idx]\n",
        "    audio_feature = audio_feature_extractor(raw_speech = audio, return_tensors = 'np', sampling_rate = SAMPLING_RATE)\n",
        "    audio_values, audio_attn_mask = audio_feature['input_values'][0], audio_feature['attention_mask'][0]\n",
        "\n",
        "    item = {\n",
        "        'label' : label\n",
        "        'audio_values' : audio_values,\n",
        "        'audio_attn_mask' : audio_attn_mask,\n",
        "    }\n",
        "\n",
        "    return item\n",
        "\n",
        "\n",
        "def collate_fn(samples):\n",
        "  batch_labels = []\n",
        "  batch_audio_values = []\n",
        "  batch_audio_attn_masks = []\n",
        "\n",
        "  for sample in samples:\n",
        "    batch_labels.append(sample['label'])\n",
        "    batch_audio_values.append(torch.tensro(sample['audio_values']))\n",
        "    batch_audio_attn_masks.append(torch.tensor(sample['audio_attn_mask']))\n",
        "\n",
        "  batch_labels = torch.tensor(batch_labels)\n",
        "  batch_audio_values = pad_sequence(batch_audio_values, batch_first = True)\n",
        "  batch_audio_attn_masks = pad_sequence(batch_audio_attn_masks, batch_first = True)\n",
        "\n",
        "  batch = {\n",
        "      'label' : batch_labels,\n",
        "      'audio_values' : batch_audio_values,\n",
        "      'audio_attn_mask' : batch_audio_attn_masks,\n",
        "  }\n",
        "\n",
        "  return batch\n",
        "\n",
        "\n",
        "class MyLitModel(pl.LightningModule):\n",
        "  def __init__(self, audio_model_name, num_labels, n_layers=1, projector=True, classifier=True, dropout=0.07, lr_decay=1):\n",
        "    super(MyLitModel, self).__init__()\n",
        "    self.config = AutoConfig.from_pretrained(audio_model_name)\n",
        "    self.config.activation_dropout = dropout\n",
        "    self.config.attention_dropout = dropout\n",
        "    self.config.final_dropout = dropout\n",
        "    self.config.hidden_dropout = dropout\n",
        "    self.config.hidden_dropout_prob = dropout\n",
        "    self.audio_model = HubertForSequenceClassification.from_pretrained(audio_model_name, config=self.config)\n",
        "    self.lr_decay = lr_decay\n",
        "    self._do_reinit(n_layers, projector, classifier)\n",
        "\n",
        "  def forward(self, audio_values, audio_attn_mask):\n",
        "    logits = self.audio_model(input_values = audio_values, attention_mask = audio_attn_mask).logits\n",
        "    logits = torch.stack([\n",
        "        logits[:, 0] + logits[:, 7],\n",
        "        logits[:, 2] + logits[:, 9],\n",
        "        logits[:, 5] + logits[:, 12],\n",
        "        logits[:, 1] + logits[:, 8],\n",
        "        logits[:, 4] + logits[:, 11],\n",
        "        logits[:, 3] + logits[:, 10],]\n",
        "        , dim = -1)\n",
        "    return logits\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    audio_values = batch['audio_values']\n",
        "    audio_attn_mask = batch['audio_attn_mask']\n",
        "    labels = batch['label']\n",
        "\n",
        "    logits = self(audio_values, audio_attn_mask)\n",
        "    loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    acc = accuracy(preds, labels)\n",
        "\n",
        "    self.log('train_loss', loss, on_step = True,  on_epoch = True, prog_bar = True, logger = True)\n",
        "    self.log('train_acc', acc, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def validation_step(self, batch, batch_idx):\n",
        "  audio_values = batch['audio_values']\n",
        "  audio_attn_mask = batch['audio_attn_mask']\n",
        "  labels = batch['label']\n",
        "\n",
        "  logits = self(audio_values, audio_attn_mask)\n",
        "  loss = nn.CrossEntropyLoss()(logits, labels)"
      ],
      "metadata": {
        "id": "4HJa2olh1Qvu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}